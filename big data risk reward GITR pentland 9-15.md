


Accepted to World Economic Forum Annual Global IT Report, copyright Alex Pentland 9/2013
Big Data: Balancing the risks and rewards of data driven public policy
Alex Pentland, MIT
pentland@mit.edu
OVERVIEW
In June 2013, massive U.S. surveil ance of phone records and Internet data was revealed by
former National Security Agency (NSA) contractor Edward Snowden, who cal ed these activities the
“architecture of oppression.” His disclosures ignited an overdue public debate on the balance between
personal privacy and our growing digital capabilities regarding the col ection and use of personal data.
Finding this balance is an issue of vital and urgent interest to corporations and governments, as well as
ordinary citizens around the world. This report will outline both the risks and the rewards of this new
age of big data, address policy issues in this area and provide practical recommendations for a way
forward.
INTRODUCTION
Data about human behavior, such as census data, have always been essential for both
government and industry. In recent years, however, a new methodology for collecting these data about
human behavior has emerged. By analyzing patterns within the `digital breadcrumbs’ that we all leave
behind us as we move through the world – call records, credit card transactions, and GPS location fixes,
etc. – scientists are discovering that we can begin to explain many things – financial crashes, revolutions,
panics – that previously appeared to be random events. These new tools, with their view of life in all its
complexity, are the future of social science and public policy. Just as the microscope and telescope
revolutionized the study of biology and astronomy, `socioscopes’ have the potential to revolutionize
regulation and public policy.
The risk of deploying this sort of data driven policy and regulation comes from the danger of
putting so much personal data in the hands of either companies or governments. Fortunately, new
approaches to regulation and technology have been developed that can help protect personal privacy
from exploitation, and can mitigate the problem of government overreach as wel . Both regulation and
technology must continue to evolve in order to provide more scientific, real-time public policy while not
exposing citizens to the dangers of exploitative companies or an al -knowing authoritarian government;
this report will provide practical recommendations to achieve these goals.
A BIG DATA TAXONOMY
It is probably hopeless to try to provide a detailed taxonomy of data types and uses because the
technology is progressing so quickly. But it is possible to provide a broad taxonomy framed in terms of
control. The three main divisions within the spectrum of data control are: (1) data commons, which are
available to all, with at most minor limitations on use; (2) personal or proprietary data, typically
control ed by individuals or companies, for which there needs to be legal and technology infrastructure
that provides strict control and auditing of use; and (3) the secret data of governments which typically
has less direct public oversight and more stringent controls. The issues with data commons will be
addressed first, followed by personal and proprietary data, and then government secret data.
The preferred lens for examining these issues is experimentation in the real world, rather than
arguments from theory or first principles, because using massive, live data to design institutions and
1
Accepted to World Economic Forum Annual Global IT Report, copyright Alex Pentland 9/2013
policies is outside of our traditional way of managing things. In this new digital era we cannot only rely
on existing policy, tradition or even laboratory science, because the strengths and weaknesses of big
data analysis are very different from standard information sources. To begin to manage our society in a
data driven manner requires us to move beyond academic debate and laboratory question-and-
answering processes. Instead, we need to try out new policy ideas within living laboratories – real,
diverse communities that are willing to try a new way of doing things – in order to test and prove our
ideas. This is new territory and so it is important for us to constantly try out new ideas in the real world
in order to see what works and what doesn’t.
DATA COMMONS
The first entry in the data taxonomy is the data commons. A key insight is that our data are
worth more when shared because they can inform improvements in systems such as public health,
transportation, and government. Using a “digital data commons” can potentially give us unprecedented
instrumentation of how our policies are performing so we can know when to take action to quickly and
effectively address the situation.
We already have many data commons available: maps, census data, and financial indices. With
the advent of big data we can potential y develop many more types of data commons, and these
commons can be both `real time’ and far more detailed than previous examples. This is because these
new digital commons depend mostly on data that are already produced as a side-effect of ongoing daily
life (e.g., digital transaction records, cel phone location fixes, road toll records, etc.) and because they
can be produced automatical y by computers without human intervention.
One major concern with such a data commons is that they can endanger personal privacy.
Another secondary concern involves the tension between commercial or personal interests: these
proprietary interests might tend to reduce the richness of such a commons, and diminish the ability of
such a data commons to enable significant public goods.
To explore the viability of a big data commons, what is perhaps the world’s first true ‘big data’
commons was unveiled on May 1, 2013. In this Data for Development (D4D) initiative, ninety research
organizations from around the world reported hundreds of results from their analysis of data describing
the mobility and call patterns of the citizens of the entire African country of Ivory Coast [1]. The data
were donated by the mobile carrier Orange, with help from the University of Louvain (Belgium) and my
lab at MIT (United States), along with collaboration from Bouake University (Ivory Coast), the United
Nation’s Global Pulse, the World Economic Forum, and the GSMA (which is the mobile carriers’
international trade association). The D4D progam was led by Nicolas De Cordes (Orange), Vincent
Blondel (Louvain), Alex Pentland (MIT), Robert Kirkpatrick (UN Global Pulse), and Bill Hoffman (World
Economic Forum).
The ninety projects that explored the use of this data commons cover many different aspects of
better governance. An example of using the D4D data to improve social equality was highlighted by
work done by researchers at the University Col ege of London, who developed a method for mapping
poverty from the diversity of cell phone usage. As people have more disposable income, their patterns
of movement and pattern of phone calls becomes increasingly diverse. Another example of using the
D4D data for social equality is the mapping of ethnic boundaries by researchers from the University of
California, San Diego. This method relies on the fact that ethnic and language groups communicate far
more within their own group than they communicate with others groups. This project is significant
because, while we know that ethic violence often erupts along such boundaries, the government and aid
agencies are usually uncertain about the geography of these social fault zones.
The D4D data was utilized to understand and promote operational efficiency through an analysis
by IBM’s Dublin laboratory of Ivory Coast’s the public transportation system. This analysis showed that
2
Accepted to World Economic Forum Annual Global IT Report, copyright Alex Pentland 9/2013
for very little cost, the average commute time in Abidjan, the Ivory Coast’s biggest city, could be cut by
10 percent. Other research groups demonstrated similar potentials for operational improvements in the
areas of government, commerce, agriculture, and finance.
Finally, examples of using D4D data to improve social resiliency include analysis of disease
spread by groups from Novi Sad University (Serbia), EPFL (Switzerland), and Birmingham (UK). These
research groups showed that smal changes in the public health system could potential y cut the spread
of flu by 20 percent as well as significantly reducing the spreads of HIV and malaria.
These selected results are just a smal sample of the impressive work that is made possible by
this rich and unique data commons. These results and others like them are available at
http://www.d4d.orange.com/home. Each of these D4D research projects has demonstrated the great
potential of a big data commons for improving society. From the point of view of Orange, it also
demonstrates the potential for new lines of business that combine this data commons with customers
personal data: Imagine phone apps that advises commuters about which bus wil get them to work
quickest, or helps citizens reduce the risk of catching the flu.
The work of these ninety research groups also suggests that many of the privacy fears
associated with the release of data about human behavior may be general y misunderstood. In this data
commons, the data were processed by advanced computer algorithms (e.g., sophisticated sampling and
use of aggregated indicators) so that it was unlikely that any individual could be re-identified. In fact, no
path to re-identification was discovered by the even though several of the research groups that studied
this specific question.
In addition, while the data were freely available for any legitimate research that a group was
interested in, the data were distributed under a legal contract that specified that they the data could
only be used for the purpose proposed and only by the specific people making the proposal. A similar
technology-legal framework is used in trust networks described in the next section. The use of both
advanced computer algorithms and contract law to specify and audit how personal data may be used
and shared is the goal of new privacy regulations in the EU, the United States, and elsewhere.
PERSONAL AND PROPRIETARY DATA
The second category in the data taxonomy is personal and proprietary data, typically controlled
by individuals or companies, for which there needs to be legal and technology infrastructure that
provides strict control and auditing of use. The current best practice is a system of data sharing called
trust networks [2,3,4]. Trust networks are a combination of a computer network that keeps track of user
permissions for each piece of personal data, and a legal contract that specifies both what can and can’t
be done with the data and what happens if there is a violation of the permissions. This is the model of
personal data management that is most frequently proposed within the World Economic Forum
Personal Data Initiative.
In such a system all personal data have attached labels specifying what the data can, and
cannot, be used for. These labels are exactly matched by terms in a legal contract between al the
participants stating penalties for not obeying the permission labels and giving the right to audit the use
of the data. Having permissions, including the provenance of the data, al ows automatic auditing of data
use and allows individuals to change their permissions and withdraw data.
Today, there are long-standing versions of trust networks that have proven to be both secure
and robust. The best known example is the SWIFT network for inter-bank money transfer and its most
distinguishing feature is that it has never been hacked. When asked why he robbed banks, bank robber
Willie Sutton famously said, `because that’s where the money is’. In today’s world, the SWIFT network is
where the money is – trillions of dollars per day. This trust network has not only kept the robbers away,
but it also makes sure the money reliably goes where it is supposed to go. Until recently such systems
3
Accepted to World Economic Forum Annual Global IT Report, copyright Alex Pentland 9/2013
were only for the `big guys’. To give individuals a similarly safe method of managing personal data, my
research group and I here at MIT, in partnership with the Institute for Data Driven Design
(http://idcubed.org), have helped build openPDS (open Personal Data Store), a consumer version of this
type of system and are now testing it with a variety of industry and government partners [4].
A major concern about trust networks is the cost associated with keeping track of permissions
and supporting the capability for automated auditing. Since many companies maintain such data
structures already in order to support internal compliance and auditing functions, the cost concern does
not seem to be a major barrier. Another more serious concern, however, is the extent to which
incidental data about human behavior must be included in the permissions and auditing framework.
Such data are typically collected in the course of normal operations in order to support those operations
(e.g., the location of a cel phone is required to complete phone cal s) but without specific informed
consent. A final concern is that a trust network system may be too complex for average people to use,
or that it will not inspire (or deserve) the sort of user trust that the name suggests.
In order to investigate these concerns, a living lab has been launched with the city of Trento in
Italy, supported by Telecom Italia, Telefonica, my MIT research group, the Fondazione Bruno Kessler, the
Institute for Data Driven Design, and local companies within Trento. Importantly, this living lab has the
approval and informed consent of all its participants – they know that they are part of a real-world
experiment whose goal is to invent a better way of living [5].
The goal of this living lab is to develop new ways of sharing data to promote greater civic
engagement and information diffusion. One specific goal is to build upon and test trust-network
software such as the openPDS system, by deploying a set of “personal data services” designed to enable
users to col ect, store, manage, disclose, share and use data about themselves. For example, the
openPDS system lets the community of young families learn from each other without the work of
entering data by hand or the risk of sharing through current social media. These data can then be used
for the personal self-empowerment of each member, or (when aggregated) for creation of a data
commons that supports improvement of the community. The ability to share data safely should enable
better idea flow among individuals, companies, and government, and we want to see if these tools can
in fact increase productivity and creative output at the scale of an entire city.
The Trento living lab will also investigate how to deal with the sensitivities of collecting and
using deeply personal data in real-world situations. For example, it wil explore different techniques and
methodologies to protect the users’ privacy while at the same time being able to use these personal
data to generate a useful data commons. It will also explore different user interfaces for privacy settings,
for configuring the data col ected, for the data disclosed to applications and for those shared with other
users, all in the context of a trust framework. While the Trento experiment is still in its early days, the
initial reaction from participating families is that these sorts of data sharing capabilities are valuable, and
they feel safe sharing their data using the openPDS system.
GOVERNMENT DATA
The third category in the taxonomy is secret government data. A major risk of deploying data
driven policies and regulations comes from the danger of putting so much personal data in the hands
governments. But how can it happen that governments, especial y authoritarian governments, choose
to limit their reach? The answer is that unlimited access to data about the citizen behavior is a great
danger to the government as well as the citizenry. Consider the NSA’s response to the recent Snowden
leaks:
“This failure originated from two practices that we need to reverse,” Ashton B. Carter, the
deputy secretary of defense, said recently. “There was an enormous amount of information concentrated
4
Accepted to World Economic Forum Annual Global IT Report, copyright Alex Pentland 9/2013
in one place,” he said. “That’s a mistake.” And second, no individual should be given the kind of access
Mr. Snowden had, Mr. Carter said.
http://www.nytimes.com/2013/08/04/sunday-review
That is, the government must organize big data resources in a distributed manner, with each
different type of data separated and dispersed among many locations, using many different types of
computer systems and encryption. Similarly, human resources should be organized into cells of access
and permission that are localized both spatial y and by data type. Both computer and human resources
should always be redundant and fragmented in order to avoid overly-powerful central actors.
The logic behind this observation is that databases which have different types of data that are
physically and logically distributed, and which also have heterogeneous computer and encryption
systems, are hard to attack, both physically as well as through cyber attack. This is because any single
exploit is likely to gain access to only a limited part of the whole database. Similarly, the resilience of
organizations with a heterogeneous cell-like human and permissions structure is familiar from
intelligence and terrorist organizations. Importantly, resistance to attack by adopting a distributed
organization is a particularly pressing issue for authoritarian governments, because unfettered access to
data about citizen behavior can be a major aid to organizing a successful coup to overthrow the
government.
What does al this have to do with the danger that a big data government will trample individual
freedoms? The key insight is that for these types of data systems, each type of data analysis operation
has a characteristic pattern of communication between different databases and human operators. As a
consequence it is possible to monitor the functioning of the data analysis process without access to, or
endangerment of, the analysis content. In short, one can use `metadata about metadata’ in order to
monitor the use of metadata, and with some reasonable confidence ensure that only `normal’ analysis
operations are being conducted without reference to specific content. Governments that structure their
data resources in this manner can more easily monitor attacks and misuse of all sorts.
As a concrete example let us assume a system in which different types of database are
physically distributed. In this case one can observe the amount and pattern of traffic between the
different databases. These patterns are characteristic of the analysis being performed and so deviations
from the normal patterns of communication between databases are cause for concern. In this manner
an open civil authority can perform substantial, fairly effective monitoring of the functioning of a
classified agency. In most cases it is sufficient to that each element of the system only monitor local
traffic. Afamiliarexampleofthistypeofmonitoringisthe`manyeyes’securitystrategy.Whenpatterns
of communication among different departments are visible (as with physical mail), then the patterns of
`normal’ operations are also visible to many employees, even though the content of the operations
remains hidden. For example, a health official responsible for maintaining health records will be able to
see if those records are suddenly being accessed by the finance records office with unusual frequency,
and may inquire if that is proper. In contrast, when copies of all the data types are all in one place, it is
easy for people to conduct unauthorized analyses.
The computer architecture for this type of system is very similar to the trust networks described
in the previous section: distributed data stores with permissions, provenance, and auditing for sharing
between data stores. In this case, however, the data stores are segmented by their referent, e.g., tax
records for individuals, tax records for companies, import records from country X to port Y, etc., rather
than having one data store per person. Because the architecture is so similar to the citizen-centric
personal data stores, it enables easier and safer sharing of data between citizens and government. For
this reason several states within the U.S. are beginning to test this architecture for both internal and
external data analysis services.
5
Accepted to World Economic Forum Annual Global IT Report, copyright Alex Pentland 9/2013
Finally, it should not escape the reader’s attention that all of these lessons also apply to
companies with large, complex databases. Misbehavior by employees, industrial espionage, and cyber
attack are among the greatest dangers that companies face in the big data era. A distributed
architecture of databases joined with a network that supports permissions, provenance, and auditing
can reduce risk and increase resilience of companies’ internal data analysis functions.
SUMMARY
We are entering a big data world, where governance is far more driven by data than in the past.
Basic to the success of a data-driven society is the protection of personal privacy and freedom.
Discussions at the World Economic Forum have made substantial contributions to altering the privacy
and data ownership standards around the world in order to give individuals unprecedented control over
data that are about them, while at the same time providing for increased transparency and engagement
in both the public and private spheres.
We still face the challenge that large organizations, and in particular governments, may be
tempted to abuse the power of the data that they hold. To address this concern we need to establish
best practices that are in the interest of both large organizations and individuals. This paper has
suggested one path in which potential abuses of power can be limited, while at the same time providing
greater security for organizations that use big data. The key policy recommendations for all large
organizations, commercial or government, are that:
1) Large data systems should store data in a distributed manner, separated by type (e.g.
financial vs. health) and real-world categories (e.g., individual vs. corporate), managed by a
department whose function is focused on that data, and with sharing permissions set and
monitored by personnel from that department. Best practice would have the custodians of
data be regional and use heterogeneous computer systems. With such safeguards in place,
it is difficult to attack many different types of data at once, and it is more difficult to
combine data types without authentic authorization.
2) Data sharing should always maintain provenance and permissions associated with data, and
support automatic, tamper-proof auditing. Best practice would share only answers to
questions about the data (e.g., by use of pre-programmed SQL queries known as `Database
Views’), rather than the data itself, whenever possible. This allows improved internal
compliance and auditing, and helps minimizes risk for unauthorized information leakage.
3) Systems controlled by partner organizations, and not just your own systems, should be
secure. External data sharing should only be between data systems that have similar local
control, permissions, provenance, and auditing, and should include the use of standardized
legal agreements such as those employed in trust networks, as described earlier. Otherwise
data can be siphoned off at the data source or end consumer, without the need for
attacking central system directly.
4) The need for a secure data ecosystem extends to the private data of individuals and the
proprietary data of partner companies. As a consequence best practice for data flows to
and from individual citizens and businesses is to require them to have secure personal data
stores and be enrolled in a trust network data sharing agreement [2,3,4].
5) Al entities should employ secure identity credentials at al times. Best practice is to base
these credentials on biometric signatures [6].
6) Create an `open’ data commons that is available to partners under a lightweight legal
agreement, such as the trust network agreements. Open data can generate great value by
allowing third parties to improve services.
6
Accepted to World Economic Forum Annual Global IT Report, copyright Alex Pentland 9/2013
While these recommendations might at first glance seem cumbersome, they are for the most
part easily implemented with the standard protocols found within modern computer databases and
networks. In many cases, the use of distributed data stores and management are already part of
current practice, and so the entire system will be simpler and cheaper to implement than a centralized
solution: all that is really new is the careful use of provenance, permissions and auditing within a legal or
regulatory framework such as a trust network. Most importantly, these recommendations will result in
a data ecosystem that is more secure and resilient, allowing us to safely reap the advantages of using big
data to help set and monitor public policy.
References
[1]
http://www.d4d.orange.com/home
[2] Pentland, A. 2009. Reality mining of mobile communications: Towards a New Deal on Data.
In The Global Information Technology Report 2008-2009: Mobility in a Networked World, eds. S. Dutta
and I. Mia, 75-80. Geneva: World Economic Forum. See
www.insead.edu/v1/gitr/wef/main/fullreport/files/Chap1/1.6.pdf
.
[3] WEF 2011, Personal data: The emergence of a new asset class. See
http://www3.weforum.org/docs/WEF_ITTC_PersonalDataNewAsset_Report_2011.pdf
[4]
http://idcubed.org
[5]
http://www.mobileterritoriallab.eu/
[6] See
http://openid.net/connect/
7
Accepted to World Economic Forum Annual Global IT Report, copyright Alex Pentland 9/2013
SPECIAL INTEREST BOX
The Future of Big Data and Governance:
The D4D data commons is only a small first step toward improving governance by use of big
data. Much more can be accomplished, because our current understanding of policy and human society
is based on very limited data resources. Currently, most social science is based on either analysis of
laboratory experiments or on survey data. These approaches also miss the critical fact that it is the
details of which people you interact with, and how you interact with them that truly matter. Social
phenomena are really made up of billions of small transactions between individuals – people trading not
only goods and money but also information, ideas, or just gossip. There are patterns in those individual
transactions that drive phenomena such as financial crashes and Arab Springs. We need to understand
these micro-patterns because they don’t just average out to the classical way of understanding society.
Big data gives us --- for the first time --- a chance to view society in all its complexity, composed of
millions of networks of person-to-person exchanges.
Living lab studies
Figure 1. Qualitative overview of social science “living labs” and traditional experiments, with
the horizontal axis showing data collection duration and the vertical axis showing richness of the
information collected. Datasets include: (1) Most social science experiments, (2) Midwest Field Station
Study, (3) Framingham Heart Study, (4) Large Call Record datasets, (5) Reality Mining, (6) Social
Evolution, (7) Friends and Family, (8) Sociometric Badge studies, (9) Data for Development (D4D)
dataset, (10) where the world is headed.
If we had a `God’s Eye’ all-seeing view, then we could potential y arrive at a true understanding
of how society works and develop scientifically proven methods to fix our problems. Unfortunately, as
illustrated in Figure 1, almost all data from traditional social science (labeled `1’) are near the (0,0)
8
Accepted to World Economic Forum Annual Global IT Report, copyright Alex Pentland 9/2013
coordinate, meaning these datasets represent information gathered from under a hundred people and
only for a few hours. The studies labeled `2’ and ‘3’ are some of the largest social science studies to date
[7,8]. In the last decade, computational social scientists have begun to discover how to leverage big data
and have been using datasets from companies such as cell phone carriers and social media firms. Typical
examples of these studies are labeled `4’.

Unfortunately, even these large data sets are impoverished because they measure only a few
variables at a time. Therefore, they provide a very limited view of human nature. Recently data scientists
have developed `living lab’ technologies for harvesting `digital breadcrumbs’ and are now obtaining
much richer descriptions of human behavior. The studies labeled 5, 6, 7 and 8 are Living Lab studies
using smart phones or electronic name badges (“sociometers”) to collect data [5,9]. The point labeled 9
is the Data for Development dataset covering the entire country of the Ivory Coast [1].
Just a brief examination of Figure 1 makes it easy to see that these living lab datasets are many
orders of magnitude richer than previous social science datasets. These large, digital datasets contain
extraordinary amounts of objective, continuous, dense data that al ow us to build quantitative,
predictive models of human behavior in complex, everyday situations.
Importantly, the point labeled `10’ is where the world is headed. In just a few short years we are
likely to have available incredibly rich data about the behavior of virtually all of humanity on a
continuous basis. The data mostly already exist in cell phone networks, credit card databases, and
elsewhere, but currently only technical gurus have access to these data. As these digital data become
more widely available for scientific inquiry we will be able to understand and manage ourselves in ways
better suited to our complex, interconnected, and networked society.
References
[7] Barker, R. 1968. Ecological psychology: Concepts and methods for studying the environment
of human behavior. Palo Alto, CA: Stanford University Press.
[8] Dawber, T. 1980. The Framingham study: The epidemiology of atherosclerotic disease.
Cambridge, MA: Harvard University Press.
[9] Pentland, A., 2014, Social Physics, Penguin Press
9
